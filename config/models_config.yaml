# Cấu hình các Foundation Models để test với Inference Profiles
foundation_models:
  # Claude Models - Sử dụng Inference Profiles
  claude_3_5_sonnet_v2:
    model_id: "us.anthropic.claude-3-5-sonnet-20241022-v2:0"  # Inference Profile
    display_name: "Claude 3.5 Sonnet v2"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.003  # per 1K tokens
      output_tokens: 0.015 # per 1K tokens
    request_format: "anthropic"
  
  claude_3_5_sonnet_v1:
    model_id: "us.anthropic.claude-3-5-sonnet-20240620-v1:0"  # Inference Profile
    display_name: "Claude 3.5 Sonnet v1"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.003  # per 1K tokens
      output_tokens: 0.015 # per 1K tokens
    request_format: "anthropic"
  
  claude_3_5_haiku:
    model_id: "us.anthropic.claude-3-5-haiku-20241022-v1:0"  # Inference Profile
    display_name: "Claude 3.5 Haiku"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.0008 # per 1K tokens
      output_tokens: 0.004  # per 1K tokens
    request_format: "anthropic"
  
  claude_3_sonnet:
    model_id: "us.anthropic.claude-3-sonnet-20240229-v1:0"  # Inference Profile
    display_name: "Claude 3 Sonnet"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.003  # per 1K tokens
      output_tokens: 0.015 # per 1K tokens
    request_format: "anthropic"
  
  claude_3_haiku:
    model_id: "us.anthropic.claude-3-haiku-20240307-v1:0"  # Inference Profile
    display_name: "Claude 3 Haiku"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.0008 # per 1K tokens
      output_tokens: 0.004  # per 1K tokens
    request_format: "anthropic"
  
  claude_3_opus:
    model_id: "us.anthropic.claude-3-opus-20240229-v1:0"  # Inference Profile
    display_name: "Claude 3 Opus"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.015  # per 1K tokens
      output_tokens: 0.075 # per 1K tokens
    request_format: "anthropic"

  # Meta Llama Models - Sử dụng Inference Profiles
  llama3_2_90b:
    model_id: "us.meta.llama3-2-90b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.2 90B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.002  # per 1K tokens
      output_tokens: 0.002 # per 1K tokens
    request_format: "llama"
  
  llama3_2_11b:
    model_id: "us.meta.llama3-2-11b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.2 11B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.00035 # per 1K tokens
      output_tokens: 0.0014  # per 1K tokens
    request_format: "llama"
  
  llama3_2_3b:
    model_id: "us.meta.llama3-2-3b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.2 3B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.0001 # per 1K tokens
      output_tokens: 0.0002 # per 1K tokens
    request_format: "llama"
  
  llama3_2_1b:
    model_id: "us.meta.llama3-2-1b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.2 1B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.0001 # per 1K tokens
      output_tokens: 0.0002 # per 1K tokens
    request_format: "llama"
  
  llama3_1_8b:
    model_id: "us.meta.llama3-1-8b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.1 8B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.0003 # per 1K tokens
      output_tokens: 0.0006 # per 1K tokens
    request_format: "llama"
  
  llama3_1_70b:
    model_id: "us.meta.llama3-1-70b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.1 70B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.00099 # per 1K tokens
      output_tokens: 0.00099 # per 1K tokens
    request_format: "llama"
  
  llama3_3_70b:
    model_id: "us.meta.llama3-3-70b-instruct-v1:0"  # Inference Profile
    display_name: "Llama 3.3 70B Instruct"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.00099 # per 1K tokens
      output_tokens: 0.00099 # per 1K tokens
    request_format: "llama"

  # Amazon Nova Models - Sử dụng Inference Profiles
  nova_lite:
    model_id: "us.amazon.nova-lite-v1:0"  # Inference Profile
    display_name: "Nova Lite"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.00006 # per 1K tokens
      output_tokens: 0.00024 # per 1K tokens
    request_format: "nova"
  
  nova_micro:
    model_id: "us.amazon.nova-micro-v1:0"  # Inference Profile
    display_name: "Nova Micro"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.000035 # per 1K tokens
      output_tokens: 0.00014 # per 1K tokens
    request_format: "nova"
  
  nova_pro:
    model_id: "us.amazon.nova-pro-v1:0"  # Inference Profile
    display_name: "Nova Pro"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.0008 # per 1K tokens
      output_tokens: 0.0032 # per 1K tokens
    request_format: "nova"
  
  nova_premier:
    model_id: "us.amazon.nova-premier-v1:0"  # Inference Profile
    display_name: "Nova Premier"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.008 # per 1K tokens
      output_tokens: 0.032 # per 1K tokens
    request_format: "nova"

  # DeepSeek Models - Sử dụng Inference Profiles
  deepseek_r1:
    model_id: "us.deepseek.r1-v1:0"  # Inference Profile
    display_name: "DeepSeek R1"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.00055 # per 1K tokens
      output_tokens: 0.0022 # per 1K tokens
    request_format: "deepseek"

  # Mistral Models - Sử dụng Inference Profiles
  mistral_pixtral_large:
    model_id: "us.mistral.pixtral-large-2502-v1:0"  # Inference Profile
    display_name: "Mistral Pixtral Large 25.02"
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    pricing:
      input_tokens: 0.003 # per 1K tokens
      output_tokens: 0.009 # per 1K tokens
    request_format: "mistral"

# Cấu hình Embedding Models (vẫn sử dụng model ID trực tiếp)
embedding_models:
  titan_embeddings_v2:
    model_id: "amazon.titan-embed-text-v2:0"
    dimensions: 1024
    pricing:
      per_1k_tokens: 0.0001
  
  cohere_embed_english:
    model_id: "cohere.embed-english-v3"
    dimensions: 1024
    pricing:
      per_1k_tokens: 0.0001

# Cấu hình Knowledge Base
knowledge_base:
  # Thay thế bằng KB ID thực tế
  kb_id: "YOUR_KNOWLEDGE_BASE_ID"
  
  # Model cho Knowledge Base (sử dụng inference profile)
  model_arn: "arn:aws:bedrock:us-east-1::inference-profile/us.anthropic.claude-3-haiku-20240307-v1:0"
  
  # Cấu hình retrieval
  retrieval_config:
    numberOfResults: 10
    
  # Cấu hình generation
  generation_config:
    inferenceConfig:
      textInferenceConfig:
        maxTokens: 2048
        temperature: 0.7
        topP: 0.9

# Cấu hình Agent
agent:
  # Thay thế bằng Agent ID thực tế
  agent_id: "YOUR_AGENT_ID"
  agent_alias_id: "TSTALIASID"
  
  # Cấu hình session
  session_config:
    sessionTtl: 3600

# Cấu hình Guardrails
guardrails:
  # Thay thế bằng Guardrail ID thực tế
  guardrail_id: "YOUR_GUARDRAIL_ID"
  guardrail_version: "DRAFT"

# Request format templates
request_formats:
  anthropic:
    anthropic_version: "bedrock-2023-05-31"
    messages: []
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
  
  llama:
    prompt: ""
    max_gen_len: 4096
    temperature: 0.7
    top_p: 0.9
  
  nova:
    messages: []
    inferenceConfig:
      max_new_tokens: 4096
      temperature: 0.7
      top_p: 0.9
  
  deepseek:
    messages: []
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
  
  mistral:
    messages: []
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
